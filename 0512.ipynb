{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a529777b-f300-4581-a05f-7b3bc943cb99",
   "metadata": {},
   "source": [
    "- 290p~"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a360451-6a4b-47c3-9078-19ad1fb95612",
   "metadata": {},
   "source": [
    "### **`회귀`**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c03dc70-9222-45f9-a56a-0e5ab9f64ca3",
   "metadata": {},
   "source": [
    "- 데이터 값이 평균과 같은 일정한 값(꼭 평균은 X)으로 돌아가려는 경향을 이용한 통계학 기법\n",
    "- 여러 개의 독립변수와 한 개의 종속변수 간의 상관관계를 모델링하고 예측\n",
    "- 이를테면\n",
    "  - 아파트의 방 개수, 방 크기, 주변 학군 등 여러 개의 독립변수에 따라 아파트 가격이라는 종속변수가 어떤 관계를 나타내는지를 모델링하고 예측하는 것\n",
    "  - 이때 방 개수, 방 크기, 주변 학군 등의 독립변수의 계수를 회귀 계수라 함.\n",
    "    - 독립변수의 값에 영향을 미침\n",
    "    - `회귀 계수의 값이 커지면 독립변수가 종속 변수에 미치는 영향도 커질 것임`\n",
    "  - 머신러닝 관점에서 보면 독립변수는 feature에 해당되며 종속변수는 결정 값임\n",
    "    - 머신러닝 회귀 예측의 핵심은 주어진 feature와 결정 값 데이터 기반에서 학습을 통해 최적의 회귀 계수를 찾아내는 것\n",
    "    - 회귀에서 가장 중요한 것은 회귀 계수임\n",
    "    |독립변수 개수|회귀계수의 결합|\n",
    "    |-|-|\n",
    "    |1개 : 단일 회귀|선형 : 선형 회귀|\n",
    "    |여러 개 : 다중 회귀|비선형 : 비선형 회귀|\n",
    "- 지도학습은 두 가지 유형으로 나뉘는데, 분류와 회귀\n",
    "  - 분류는 예측값이 카테고리와 같은 이산형 클래스 값\n",
    "  - 회귀는 예측값이 연속형 숫자값\n",
    "- 여러 가지 회귀 중에서 선형 회귀가 가장 많이 사용되는데 선형 회귀는 실제 값과 예측값의 차이 즉, 오류의 제곱 값(이때 오차는 플러스가 될 수도 있고 마이너스가 될 수도 있기 때문에 그냥 제곱해서 처리하는 거라고 생각 가능, 분산 구할 때도 편차의 평균을 내는 게 아니라 편차 제곱의 평균을 내듯)을 최소화하는 직선형 회귀선을 최적화하는 방식이다. \n",
    "- 선형 회귀 모델은 규제 방법에 따라 별도의 유형으로 나눌 수 있음\n",
    "  - `규제??`\n",
    "    - 일반적인 선형 회귀의 과적합 문제를 해결하기 위해서 `회귀 계수에 페널티 값을 적용`하는 것을 말함\n",
    "- 대표적인 선형 회귀 모델?\n",
    "  - `일반 선형 회귀` : 예측값과 실제 값의 RSS(Residual Sum Of Squares)를 최소화할 수 있도록 회귀 계수를 최적화하며, 규제는 적용하지 않음\n",
    "  - `릿지` : 선형 회귀에 L2 규제(상대적으로 큰 회귀 계수 값의 예측 영향도를 감소시키기 위해서 회귀 계수값을 더 작게 만드는 규제 모델) 추가 \n",
    "  - `라쏘` : 선형 회귀에 L1 규제(예측 영향력이 작은 feature의 회귀 계수를 0으로 만들어 회귀 예측 시 feature가 선택되지 않게 하는 것, 이러한 특성 때문에 L1 규제는 feature 선택 기능으로도 불림) 추가\n",
    "  - `엘라스틱넷` : L1,L2 규제를 함께 결합, 주로 feature가 많은 데이터 세트에서 적용. L1 규제로 feature의 개수를 줄임과 동시에 L2규제로 계수 값의 크기를 조정\n",
    "  - `로지스틱 회귀` : 회귀라는 이름이 붙긴 했지만, 사실 분류에 사용하는 모델임. 이진 분류뿐만 아니라 희소 영역의 분류 이를테면 텍스트 분류와 같은 영역에서도 활용 가능"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beaaf2fe-c89f-4cba-bb98-67b4da844626",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "435c3c61-ad3e-493a-87a8-f7f85e52fcdf",
   "metadata": {},
   "source": [
    "- 단순 선형 회귀를 통한 회귀 이해\n",
    "  - 단순 선형 회귀 : 독립변수도 하나 종속변수도 하나인 선형 회귀\n",
    "  - 최적의 회귀 모델을 만든다는 것 = 전체 데이터의 잔차 합이 최소가 되는 모델을 만든다는 의미\n",
    "    - 이때 잔차는 제곱해서 처리하겠지?\n",
    "    - 혹은 절댓값을 취하기도 함\n",
    "    - 제곱하는 방법은 RSS라 부름\n",
    "      - 미분할 때 용이\n",
    "  - 동시에 오류 값 합이 최소가 될 수 있는 최적의 회귀 계수를 찾는다는 의미도 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b0eb37f-53e5-484a-80c4-9ece158320d2",
   "metadata": {},
   "source": [
    "- ### RSS\n",
    "  - 변수가 $w_0,w_1$인 식으로 표현할 수 있음\n",
    "  - 이 RSS를 최소로 하는 $w_0,w_1$, 즉 회귀 계수를 학습을 통해서 찾는 것이 머신러닝 기반 회귀의 핵심 사항\n",
    "  - 이때 RSS는 당연히 실제값과 예측값의 차이이기 때문에 종속변수와 독립변수도 포함되게 되는데, 회귀 계수인 $w$가 중심 변수이다. \n",
    "  - 학습 데이터로 입력되는 독립변수와 종속변수는 RSS에서 모두 상수로 간주한다. \n",
    "  - 일반적으로 RSS는 학습 데이터의 건수로 나누어서 정규화된 식으로 표현함"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c020270-edb9-409f-ac4a-a432f1bd8b13",
   "metadata": {},
   "source": [
    "- 회귀에서 이 RSS는 비용이며, $w$변수로 구성되는 RSS를 비용 함수라고 한다. \n",
    "- 머신 러닝 회귀 알고리즘은 데이터를 계속 학습하면서 이 비용 함수가 반환하는 값 즉, 오류 값을 지속해서 감소시키고 최종적으로는 더 이상 감소하지 않는 최소의 오류 값을 구하는 것\n",
    "- 비용함수를 손실함수라고도 함"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbdd4ff8-a8b1-474a-bcd0-f32b526ddac4",
   "metadata": {},
   "source": [
    "- 비용 최소화하기 - 경사하강법(Gradient Descent) 소개\n",
    "  - 점진적으로 반복적인 계산을 통해 $W$ 파라미터 값을 업데이트하면서 오류 값이 최소가 되는 $W$ 파라미터를 구하는 방식"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b19afcb5-db64-4ba6-bf6d-214702476567",
   "metadata": {},
   "source": [
    "- 아무것도 보이지 않는 깜깜한 밤, 산 정상에서 아래로 내려갈 때 발을 뻗어 현재 위치보다 무조건 낮은 곳으로 계속 이동하는 것.\n",
    "- 반복적으로 비용 함수의 반환 값이 작아지는 방향성을 가지고 $W$파라미터를 지속해서 보정해 나간다. \n",
    "  - 그리고 오류 값이 더 이상 작아지지 않으면 그 오류 값을 최소 비용으로 판단하고 그때의 $W$ 값을 최적 파라미터로 반환"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2699b08d-f207-4fd0-93d9-4f951ad45b40",
   "metadata": {},
   "source": [
    "- 핵심\n",
    "  - 어떻게 하면 오류가 작아지는 방향으로 $W$값을 보정할 수 있을까?\n",
    "  - 포물선 형태의 2차 함수의 최저점은 해당 2차 함수의 미분 값인 1차 함수의 기울기가 가장 최소일 때다. \n",
    "  - 예를 들어 비용 함수가 포물선 형태의 2차 함수라면 경사 하강법은 최초 $w$에서부터 미분을 적용한 뒤 이 미분 값이 계속 감소하는 방향으로 순차적으로 $w$를 업데이트한다.\n",
    "    - 보통 회귀 계수가 하나는 아닐테니 편미분을 적용\n",
    "      - `편미분?`\n",
    "        - `다변수 함수의 특정 변수를 제외한 나머지 변수를 상수로 간주하여 미분하는 것`\n",
    "        - `음미분과는 다름`\n",
    "  - 마침내 더 이상 미분된 1차 함수의 기울기가 감소하지 않는 지점을 비용 함수가 최소인 지점으로 간주하고 그때의 $w$를 반환한다. \n",
    "    - 기울기가 작을 수록 그 함수의 값 자체도 가장 작을 것임\n",
    "    - 이차 함수도 기울기가 가장 작은 0에서 그 함수의 최솟값을 가짐 \n",
    "    - 물론 아래로 볼록일 때의 얘기임\n",
    "  - 편미분 결괏값들을 반복적으로 보정하면서 $w_0,w_1$값을 업데이트하면 비용 함수가 최소가 되는 $w_0,w_1$값을 찾을 수 있다. \n",
    "  - 업데이트는 새로운 $w_1$을 이전 $w_1$에서 편미분 결괏값을 마이너스하면서 적용한다. \n",
    "  - 편미분 값이 너무 클 수 있으므로 보정 계수 $\\eta$를 곱하는데 이를 학습률이라고 함"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2181500e-22ce-4afc-a760-e308b0f4c120",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f25814-8fc2-497f-a2dc-af2adeb64b37",
   "metadata": {},
   "source": [
    "296p~"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
